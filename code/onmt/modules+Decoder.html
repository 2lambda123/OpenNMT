<p><a name="onmt.Decoder.dok"></a></p>

<h2>onmt.Decoder</h2>

<p>Unit to decode a sequence of output tokens.</p>

<pre><code> .      .      .             .
 |      |      |             |
h_1 =&gt; h_2 =&gt; h_3 =&gt; ... =&gt; h_n
 |      |      |             |
 .      .      .             .
 |      |      |             |
h_1 =&gt; h_2 =&gt; h_3 =&gt; ... =&gt; h_n
 |      |      |             |
 |      |      |             |
x_1    x_2    x_3           x_n
</code></pre>

<p>Inherits from <a href="onmt+modules+Sequencer">onmt.Sequencer</a>.</p>

<p><a name="onmt.Decoder"></a></p>

<h3>onmt.Decoder(args, inputNetwork, generator, attentionModel)</h3>

<p>Construct a decoder layer.</p>

<p>Parameters:</p>

<ul>
<li><code>args</code> - module arguments</li>
<li><code>inputNetwork</code> - input nn module.</li>
<li><code>generator</code> - an output generator.</li>
<li><code>attentionModel</code> - attention model to apply on source.</li>
</ul>

<p><a name="onmt.Decoder.load"></a></p>

<h3>onmt.Decoder.load(pretrained)</h3>

<p>Return a new Decoder using the serialized data <code>pretrained</code>. 
<a name="onmt.Decoder:serialize"></a></p>

<h3>onmt.Decoder:serialize()</h3>

<p>Return data to serialize. 
<a name="onmt.Decoder:maskPadding"></a></p>

<h3>onmt.Decoder:maskPadding(sourceSizes, sourceLength)</h3>

<p>Mask padding means that the attention-layer is constrained to
  give zero-weight to padding. This is done by storing a reference
  to the softmax attention-layer.</p>

<p>Parameters:</p>

<ul>
<li>See  <a href="onmt+modules+MaskedSoftmax">onmt.MaskedSoftmax</a>.</li>
</ul>

<p><a name="onmt.Decoder:forwardOne"></a></p>

<h3>onmt.Decoder:forwardOne(input, prevStates, context, prevOut, t)</h3>

<p>Run one step of the decoder.</p>

<p>Parameters:</p>

<ul>
<li><code>input</code> - input to be passed to inputNetwork.</li>
<li><code>prevStates</code> - stack of hidden states (batch x layers*model x rnnSize)</li>
<li><code>context</code> - encoder output (batch x n x rnnSize)</li>
<li><code>prevOut</code> - previous distribution (batch x #words)</li>
<li><code>t</code> - current timestep</li>
</ul>

<p>Returns:</p>

<ol>
<li><code>out</code> - Top-layer hidden state.</li>
<li><code>states</code> - All states.</li>
</ol>

<p><a name="onmt.Decoder:forward"></a></p>

<h3>onmt.Decoder:forward(batch, encoderStates, context)</h3>

<p>Compute all forward steps.</p>

<p>Parameters:</p>

<ul>
<li><code>batch</code> - a <code>Batch</code> object.</li>
<li><code>encoderStates</code> - a batch of initial decoder states (optional) [0]</li>
<li><p><code>context</code> - the context to apply attention to.</p>

<p>Returns: Table of top hidden state for each timestep.</p></li>
</ul>

<p><a name="onmt.Decoder:backward"></a></p>

<h3>onmt.Decoder:backward(batch, outputs, criterion)</h3>

<p>Compute the backward update.</p>

<p>Parameters:</p>

<ul>
<li><code>batch</code> - a <code>Batch</code> object</li>
<li><code>outputs</code> - expected outputs</li>
<li><p><code>criterion</code> - a single target criterion object</p>

<p>Note: This code runs both the standard backward and criterion forward/backward.</p>

<h2>It returns both the gradInputs and the loss.</h2>

<p><a name="onmt.Decoder:computeLoss"></a></p></li>
</ul>

<h3>onmt.Decoder:computeLoss(batch, encoderStates, context, criterion)</h3>

<p>Compute the loss on a batch.</p>

<p>Parameters:</p>

<ul>
<li><code>batch</code> - a <code>Batch</code> to score.</li>
<li><code>encoderStates</code> - initialization of decoder.</li>
<li><code>context</code> - the attention context.</li>
<li><code>criterion</code> - a pointwise criterion.</li>
</ul>

<p><a name="onmt.Decoder:computeScore"></a></p>

<h3>onmt.Decoder:computeScore(batch, encoderStates, context)</h3>

<p>Compute the score of a batch.</p>

<p>Parameters:</p>

<ul>
<li><code>batch</code> - a <code>Batch</code> to score.</li>
<li><code>encoderStates</code> - initialization of decoder.</li>
<li><code>context</code> - the attention context.</li>
</ul>

<h4>Undocumented methods</h4>

<p><a name="onmt.Decoder.declareOpts"></a></p>

<ul>
<li><code>onmt.Decoder.declareOpts(cmd)</code>
<a name="onmt.Decoder:returnIndividualLosses"></a></li>
<li><code>onmt.Decoder:returnIndividualLosses(enable)</code>
<a name="onmt.Decoder:resetPreallocation"></a></li>
<li><code>onmt.Decoder:resetPreallocation()</code>
<a name="onmt.Decoder:findAttentionModel"></a></li>
<li><code>onmt.Decoder:findAttentionModel()</code>
<a name="onmt.Decoder:forwardAndApply"></a></li>
<li><code>onmt.Decoder:forwardAndApply(batch, encoderStates, context, func)</code></li>
</ul>
