<p><a name="onmt.Optim.dok"></a></p>

<h2>onmt.Optim</h2>

<hr>

<p><a name="onmt.Optim:updateLearningRate"></a></p>

<h3>onmt.Optim:updateLearningRate(score, epoch)</h3>

<p>decay learning rate if val perf does not improve or we hit the startDecayAt limit</p>

<h4>Undocumented methods</h4>

<p><a name="onmt.Optim.declareOpts"></a></p>

<ul>
<li><code>onmt.Optim.declareOpts(cmd)</code>
<a name="onmt.Optim"></a></li>
<li><code>onmt.Optim(args, optimStates)</code>
<a name="onmt.Optim:setOptimStates"></a></li>
<li><code>onmt.Optim:setOptimStates(num)</code>
<a name="onmt.Optim:zeroGrad"></a></li>
<li><code>onmt.Optim:zeroGrad(gradParams)</code>
<a name="onmt.Optim:prepareGrad"></a></li>
<li><code>onmt.Optim:prepareGrad(gradParams)</code>
<a name="onmt.Optim:updateParams"></a></li>
<li><code>onmt.Optim:updateParams(params, gradParams)</code>
<a name="onmt.Optim:getLearningRate"></a></li>
<li><code>onmt.Optim:getLearningRate()</code>
<a name="onmt.Optim:getStates"></a></li>
<li><code>onmt.Optim:getStates()</code></li>
</ul>
