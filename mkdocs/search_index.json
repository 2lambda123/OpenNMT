{
    "docs": [
        {
            "location": "/", 
            "text": "Sequence-to-Sequence Learning with Attentional Neural Networks\n\n\nTorch\n implementation of a standard sequence-to-sequence model with (optional)\nattention where the encoder-decoder are LSTMs. Encoder can be a bidirectional LSTM.\nAdditionally has the option to use characters\n(instead of input word embeddings) by running a convolutional neural network followed by a\n\nhighway network\n over character embeddings to use as inputs.\n\n\nThe attention model is from\n\nEffective Approaches to Attention-based\nNeural Machine Translation\n,\nLuong et al. EMNLP 2015. We use the \nglobal-general-attention\n model with the\n\ninput-feeding\n approach from the paper. Input-feeding is optional and can be turned off.\n\n\nThe character model is from \nCharacter-Aware Neural\nLanguage Models\n, Kim et al. AAAI 2016.\n\n\nThere are a lot of additional options on top of the baseline model, mainly thanks to the fantastic folks \nat \nSYSTRAN\n. Specifically, there are functionalities which implement:\n\n \nEffective Approaches to Attention-based Neural Machine Translation\n. Luong et al., EMNLP 2015.\n\n \nCharacter-based Neural Machine Translation\n. Costa-Jussa and Fonollosa, ACL 2016.\n\n \nCompression of Neural Machine Translation Models via Pruning\n. See et al., COLING 2016.\n\n \nSequence-Level Knowledge Distillation\n. Kim and Rush., EMNLP 2016.\n\n \nDeep Recurrent Models with Fast Forward Connections for Neural Machine Translation\n.\nZhou et al, TACL 2016.\n\n \nGuided Alignment Training for Topic-Aware Neural Machine Translation\n. Chen et al., arXiv:1607.01628.\n* \nLinguistic Input Features Improve Neural Machine Translation\n. Senrich et al., arXiv:1606.02892\n\n\nSee below for more details on how to use them.\n\n\nThis project is maintained by \nYoon Kim\n.\nFeel free to post any questions/issues on the issues page.\n\n\n\n\nDependencies\n\n\n\n\nLua\n\n\nYou will need the following packages:\n\n nn\n\n nngraph\n\n\nGPU usage will additionally require:\n\n cutorch\n\n cunn\n\n\nIf running the character model, you should also install:\n\n cudnn\n\n luautf8\n\n\n\n\nQuickstart\n\n\nWe are going to be working with some example data in \ndata/\n folder.\nFirst run the data-processing code\n\n\nth preprocess.lua -train_src_file data/src-train.txt -train_targ_file data/targ-train.txt\n    -valid_src_file data/src-val.txt -valid_targ_file data/targ-val.txt -output_file data/demo\n\n\n\n\nThis will take the source/target train/valid files (\nsrc-train.txt, targ-train.txt,\nsrc-val.txt, targ-val.txt\n) and build the following files:\n\n\n\n\ndemo.src.dict\n: Dictionary of source vocab to index mappings.\n\n\ndemo.targ.dict\n: Dictionary of target vocab to index mappings.\n\n\ndemo-train.t7\n: serialized torch file containing vocabulary, training and validation data\n\n\n\n\nThe \n*.dict\n files are needed to check vocabulary, or to preprocess data with fixed vocabularies\n\n\nNow run the model\n\n\nth train.lua -data_file data/demo-train.t7 -savefile demo-model\n\n\n\n\nThis will run the default model, which consists of a 2-layer LSTM with 500 hidden units\non both the encoder/decoder.\nYou can also add \n-gpuid 1\n to use (say) GPU 1 in the cluster.\n\n\nNow you have a model which you can use to predict on new data. To do this we are\ngoing to be running beam search\n\n\nth evaluate.lua -model demo-model_final.t7 -src_file data/src-val.txt -output_file pred.txt\n-src_dict data/demo.src.dict -targ_dict data/demo.targ.dict\n\n\n\n\nThis will output predictions into \npred.txt\n. The predictions are going to be quite terrible,\nas the demo dataset is small. Try running on some larger datasets! For example you can download\nmillions of parallel sentences for \ntranslation\n\nor \nsummarization\n.\n\n\n\n\nUsing additional input features\n\n\nLinguistic Input Features Improve Neural Machine Translation\n (Senrich et al. 2016) shows that translation performance can be increased by using additional input features.\n\n\nSimilarly to this work, you can annotate each word in the \nsource\n text by using the \n-|-\n separator:\n\n\nword1-|-feat1-|-feat2 word2-|-feat1-|-feat2\n\n\n\n\nIt supports an arbitrary number of features with arbitrary labels. However, all input words must have the \nsame\n number of annotations. See for example \ndata/src-train-case.txt\n which annotates each word with the case information.\n\n\nTo evaluate the model, the option \n-feature_dict_prefix\n is required on \nevaluate.lua\n which points to the prefix of the features dictionnaries generated during the preprocessing.\n\n\n\n\nPruning a model\n\n\nCompression of Neural Machine Translation Models via Pruning\n (See et al. 2016) shows that a model can be aggressively pruned while keeping the same performace.\n\n\nTo prune a model - you can use \nprune.lua\n which implement class-bind, and class-uniform pruning technique from the paper.\n\n\n\n\nmodel\n: the model to prune\n\n\nsavefile\n: name of the pruned model\n\n\ngpuid\n: Which gpu to use. -1 = use CPU. Depends if the model is serialized for GPU or CPU\n\n\nratio\n: pruning rate\n\n\nprune\n: pruning technique \nblind\n or \nuniform\n, by default \nblind\n\n\n\n\nnote that the pruning cut connection with lowest weight in the linear models by using a boolean mask. The size of the file is a little larger since it stores the actual full matrix and the binary mask.\n\n\nModels can be retrained - typically you can recover full capacity of a model pruned at 60% or even 80% by few epochs of additional trainings.\n\n\n\n\nSwitching between GPU/CPU models\n\n\nBy default, the model will always save the final model as a CPU model, but it will save the\nintermediate models as a CPU/GPU model depending on how you specified \n-gpuid\n.\nIf you want to run beam search on the CPU with an intermediate model trained on the GPU,\nyou can use \nconvert_to_cpu.lua\n to convert the model to CPU and run beam search.\n\n\n\n\nGPU memory requirements/Training speed\n\n\nTraining large sequence-to-sequence models can be memory-intensive. Memory requirements will\ndependent on batch size, maximum sequence length, vocabulary size, and (obviously) model size.\nHere are some benchmark numbers on a GeForce GTX Titan X.\n(assuming batch size of 64, maximum sequence length of 50 on both the source/target sequence,\nvocabulary size of 50000, and word embedding size equal to rnn size):\n\n\n(\nprealloc = 0\n)\n\n 1-layer, 100 hidden units: 0.7G, 21.5K tokens/sec\n\n 1-layer, 250 hidden units: 1.4G, 14.1K tokens/sec\n\n 1-layer, 500 hidden units: 2.6G, 9.4K tokens/sec\n\n 2-layers, 500 hidden units: 3.2G, 7.4K tokens/sec\n* 4-layers, 1000 hidden units: 9.4G, 2.5K tokens/sec\n\n\nThanks to some fantastic work from folks at \nSYSTRAN\n, turning \nprealloc\n on\nwill lead to much more memory efficient training\n\n\n(\nprealloc = 1\n)\n\n 1-layer, 100 hidden units: 0.5G, 22.4K tokens/sec\n\n 1-layer, 250 hidden units: 1.1G, 14.5K tokens/sec\n\n 1-layer, 500 hidden units: 2.1G, 10.0K tokens/sec\n\n 2-layers, 500 hidden units: 2.3G, 8.2K tokens/sec\n* 4-layers, 1000 hidden units: 6.4G, 3.3K tokens/sec\n\n\nTokens/sec refers to total (i.e. source + target) tokens processed per second.\nIf using different batch sizes/sequence length, you should (linearly) scale\nthe above numbers accordingly. You can make use of memory on multiple GPUs by using\n\n-gpuid2\n option in \ntrain.lua\n. This will put the encoder on the GPU specified by\n\n-gpuid\n, and the decoder on the GPU specified by \n-gpuid2\n.\n\n\n\n\nEvaluation\n\n\nFor translation, evaluation via BLEU can be done by taking the output from \nbeam.lua\n and using the\n\nmulti-bleu.perl\n script from \nMoses\n. For example\n\n\nperl multi-bleu.perl gold.txt \n pred.txt\n\n\n\n\n\n\nEvaluation of States and Attention\n\n\nattention_extraction.lua can be used to extract the attention and the LSTM states. It uses the following (required) options:\n\n\n\n\nmodel\n: Path to model .t7 file.\n\n\nsrc_file\n: Source sequence to decode (one line per sequence).\n\n\ntarg_file\n: True target sequence.\n\n\nsrc_dict\n: Path to source vocabulary (\n*.src.dict\n file from \npreprocess.py\n).\n\n\ntarg_dict\n: Path to target vocabulary (\n*.targ.dict\n file from \npreprocess.py\n).\n\n\n\n\nOutput of the script are two files, \nencoder.hdf5\n and \ndecoder.hdf5\n. The encoder contains the states for every layer of the encoder LSTM and the offsets for the start of each source sentence. The decoder contains the states for the decoder LSTM layers and the offsets for the start of gold sentence. It additionally contains the attention for each time step (if the model uses attention).\n\n\n\n\nPre-trained models\n\n\nWe've uploaded English \n-\n German models trained on 4 million sentences from\n\nWorkshop on Machine Translation 2015\n.\nDownload link is below:\n\n\nhttps://drive.google.com/open?id=0BzhmYioWLRn_aEVnd0ZNcWd0Y2c\n\n\nThese models are 4-layer LSTMs with 1000 hidden units and essentially replicates the results from\n\nEffective Approaches to Attention-based\nNeural Machine Translation\n,\nLuong et al. EMNLP 2015.\n\n\n\n\nAcknowledgments\n\n\nOur implementation utilizes code from the following:\n\n \nAndrej Karpathy's char-rnn repo\n\n\n \nWojciech Zaremba's lstm repo\n\n* \nElement rnn library\n\n\n\n\nLicence\n\n\nMIT", 
            "title": "Home"
        }, 
        {
            "location": "/#sequence-to-sequence-learning-with-attentional-neural-networks", 
            "text": "Torch  implementation of a standard sequence-to-sequence model with (optional)\nattention where the encoder-decoder are LSTMs. Encoder can be a bidirectional LSTM.\nAdditionally has the option to use characters\n(instead of input word embeddings) by running a convolutional neural network followed by a highway network  over character embeddings to use as inputs.  The attention model is from Effective Approaches to Attention-based\nNeural Machine Translation ,\nLuong et al. EMNLP 2015. We use the  global-general-attention  model with the input-feeding  approach from the paper. Input-feeding is optional and can be turned off.  The character model is from  Character-Aware Neural\nLanguage Models , Kim et al. AAAI 2016.  There are a lot of additional options on top of the baseline model, mainly thanks to the fantastic folks \nat  SYSTRAN . Specifically, there are functionalities which implement:   Effective Approaches to Attention-based Neural Machine Translation . Luong et al., EMNLP 2015.   Character-based Neural Machine Translation . Costa-Jussa and Fonollosa, ACL 2016.   Compression of Neural Machine Translation Models via Pruning . See et al., COLING 2016.   Sequence-Level Knowledge Distillation . Kim and Rush., EMNLP 2016.   Deep Recurrent Models with Fast Forward Connections for Neural Machine Translation .\nZhou et al, TACL 2016.   Guided Alignment Training for Topic-Aware Neural Machine Translation . Chen et al., arXiv:1607.01628.\n*  Linguistic Input Features Improve Neural Machine Translation . Senrich et al., arXiv:1606.02892  See below for more details on how to use them.  This project is maintained by  Yoon Kim .\nFeel free to post any questions/issues on the issues page.", 
            "title": "Sequence-to-Sequence Learning with Attentional Neural Networks"
        }, 
        {
            "location": "/#dependencies", 
            "text": "", 
            "title": "Dependencies"
        }, 
        {
            "location": "/#lua", 
            "text": "You will need the following packages:  nn  nngraph  GPU usage will additionally require:  cutorch  cunn  If running the character model, you should also install:  cudnn  luautf8", 
            "title": "Lua"
        }, 
        {
            "location": "/#quickstart", 
            "text": "We are going to be working with some example data in  data/  folder.\nFirst run the data-processing code  th preprocess.lua -train_src_file data/src-train.txt -train_targ_file data/targ-train.txt\n    -valid_src_file data/src-val.txt -valid_targ_file data/targ-val.txt -output_file data/demo  This will take the source/target train/valid files ( src-train.txt, targ-train.txt,\nsrc-val.txt, targ-val.txt ) and build the following files:   demo.src.dict : Dictionary of source vocab to index mappings.  demo.targ.dict : Dictionary of target vocab to index mappings.  demo-train.t7 : serialized torch file containing vocabulary, training and validation data   The  *.dict  files are needed to check vocabulary, or to preprocess data with fixed vocabularies  Now run the model  th train.lua -data_file data/demo-train.t7 -savefile demo-model  This will run the default model, which consists of a 2-layer LSTM with 500 hidden units\non both the encoder/decoder.\nYou can also add  -gpuid 1  to use (say) GPU 1 in the cluster.  Now you have a model which you can use to predict on new data. To do this we are\ngoing to be running beam search  th evaluate.lua -model demo-model_final.t7 -src_file data/src-val.txt -output_file pred.txt\n-src_dict data/demo.src.dict -targ_dict data/demo.targ.dict  This will output predictions into  pred.txt . The predictions are going to be quite terrible,\nas the demo dataset is small. Try running on some larger datasets! For example you can download\nmillions of parallel sentences for  translation \nor  summarization .", 
            "title": "Quickstart"
        }, 
        {
            "location": "/#using-additional-input-features", 
            "text": "Linguistic Input Features Improve Neural Machine Translation  (Senrich et al. 2016) shows that translation performance can be increased by using additional input features.  Similarly to this work, you can annotate each word in the  source  text by using the  -|-  separator:  word1-|-feat1-|-feat2 word2-|-feat1-|-feat2  It supports an arbitrary number of features with arbitrary labels. However, all input words must have the  same  number of annotations. See for example  data/src-train-case.txt  which annotates each word with the case information.  To evaluate the model, the option  -feature_dict_prefix  is required on  evaluate.lua  which points to the prefix of the features dictionnaries generated during the preprocessing.", 
            "title": "Using additional input features"
        }, 
        {
            "location": "/#pruning-a-model", 
            "text": "Compression of Neural Machine Translation Models via Pruning  (See et al. 2016) shows that a model can be aggressively pruned while keeping the same performace.  To prune a model - you can use  prune.lua  which implement class-bind, and class-uniform pruning technique from the paper.   model : the model to prune  savefile : name of the pruned model  gpuid : Which gpu to use. -1 = use CPU. Depends if the model is serialized for GPU or CPU  ratio : pruning rate  prune : pruning technique  blind  or  uniform , by default  blind   note that the pruning cut connection with lowest weight in the linear models by using a boolean mask. The size of the file is a little larger since it stores the actual full matrix and the binary mask.  Models can be retrained - typically you can recover full capacity of a model pruned at 60% or even 80% by few epochs of additional trainings.", 
            "title": "Pruning a model"
        }, 
        {
            "location": "/#switching-between-gpucpu-models", 
            "text": "By default, the model will always save the final model as a CPU model, but it will save the\nintermediate models as a CPU/GPU model depending on how you specified  -gpuid .\nIf you want to run beam search on the CPU with an intermediate model trained on the GPU,\nyou can use  convert_to_cpu.lua  to convert the model to CPU and run beam search.", 
            "title": "Switching between GPU/CPU models"
        }, 
        {
            "location": "/#gpu-memory-requirementstraining-speed", 
            "text": "Training large sequence-to-sequence models can be memory-intensive. Memory requirements will\ndependent on batch size, maximum sequence length, vocabulary size, and (obviously) model size.\nHere are some benchmark numbers on a GeForce GTX Titan X.\n(assuming batch size of 64, maximum sequence length of 50 on both the source/target sequence,\nvocabulary size of 50000, and word embedding size equal to rnn size):  ( prealloc = 0 )  1-layer, 100 hidden units: 0.7G, 21.5K tokens/sec  1-layer, 250 hidden units: 1.4G, 14.1K tokens/sec  1-layer, 500 hidden units: 2.6G, 9.4K tokens/sec  2-layers, 500 hidden units: 3.2G, 7.4K tokens/sec\n* 4-layers, 1000 hidden units: 9.4G, 2.5K tokens/sec  Thanks to some fantastic work from folks at  SYSTRAN , turning  prealloc  on\nwill lead to much more memory efficient training  ( prealloc = 1 )  1-layer, 100 hidden units: 0.5G, 22.4K tokens/sec  1-layer, 250 hidden units: 1.1G, 14.5K tokens/sec  1-layer, 500 hidden units: 2.1G, 10.0K tokens/sec  2-layers, 500 hidden units: 2.3G, 8.2K tokens/sec\n* 4-layers, 1000 hidden units: 6.4G, 3.3K tokens/sec  Tokens/sec refers to total (i.e. source + target) tokens processed per second.\nIf using different batch sizes/sequence length, you should (linearly) scale\nthe above numbers accordingly. You can make use of memory on multiple GPUs by using -gpuid2  option in  train.lua . This will put the encoder on the GPU specified by -gpuid , and the decoder on the GPU specified by  -gpuid2 .", 
            "title": "GPU memory requirements/Training speed"
        }, 
        {
            "location": "/#evaluation", 
            "text": "For translation, evaluation via BLEU can be done by taking the output from  beam.lua  and using the multi-bleu.perl  script from  Moses . For example  perl multi-bleu.perl gold.txt   pred.txt", 
            "title": "Evaluation"
        }, 
        {
            "location": "/#evaluation-of-states-and-attention", 
            "text": "attention_extraction.lua can be used to extract the attention and the LSTM states. It uses the following (required) options:   model : Path to model .t7 file.  src_file : Source sequence to decode (one line per sequence).  targ_file : True target sequence.  src_dict : Path to source vocabulary ( *.src.dict  file from  preprocess.py ).  targ_dict : Path to target vocabulary ( *.targ.dict  file from  preprocess.py ).   Output of the script are two files,  encoder.hdf5  and  decoder.hdf5 . The encoder contains the states for every layer of the encoder LSTM and the offsets for the start of each source sentence. The decoder contains the states for the decoder LSTM layers and the offsets for the start of gold sentence. It additionally contains the attention for each time step (if the model uses attention).", 
            "title": "Evaluation of States and Attention"
        }, 
        {
            "location": "/#pre-trained-models", 
            "text": "We've uploaded English  -  German models trained on 4 million sentences from Workshop on Machine Translation 2015 .\nDownload link is below:  https://drive.google.com/open?id=0BzhmYioWLRn_aEVnd0ZNcWd0Y2c  These models are 4-layer LSTMs with 1000 hidden units and essentially replicates the results from Effective Approaches to Attention-based\nNeural Machine Translation ,\nLuong et al. EMNLP 2015.", 
            "title": "Pre-trained models"
        }, 
        {
            "location": "/#acknowledgments", 
            "text": "Our implementation utilizes code from the following:   Andrej Karpathy's char-rnn repo    Wojciech Zaremba's lstm repo \n*  Element rnn library", 
            "title": "Acknowledgments"
        }, 
        {
            "location": "/#licence", 
            "text": "MIT", 
            "title": "Licence"
        }, 
        {
            "location": "/Quickstart/", 
            "text": "Quickstart\n\n\nWe are going to be working with some example data in \ndata/\n folder.\nFirst run the data-processing code\n\n\nth preprocess.lua -train_src_file data/src-train.txt -train_targ_file data/targ-train.txt\n    -valid_src_file data/src-val.txt -valid_targ_file data/targ-val.txt -output_file data/demo\n\n\n\n\nThis will take the source/target train/valid files (\nsrc-train.txt, targ-train.txt,\nsrc-val.txt, targ-val.txt\n) and build the following files:\n\n\n\n\ndemo.src.dict\n: Dictionary of source vocab to index mappings.\n\n\ndemo.targ.dict\n: Dictionary of target vocab to index mappings.\n\n\ndemo-train.t7\n: serialized torch file containing vocabulary, training and validation data\n\n\n\n\nThe \n*.dict\n files are needed to check vocabulary, or to preprocess data with fixed vocabularies\n\n\nNow run the model\n\n\nth train.lua -data_file data/demo-train.t7 -savefile demo-model\n\n\n\n\nThis will run the default model, which consists of a 2-layer LSTM with 500 hidden units\non both the encoder/decoder.\nYou can also add \n-gpuid 1\n to use (say) GPU 1 in the cluster.\n\n\nNow you have a model which you can use to predict on new data. To do this we are\ngoing to be running beam search\n\n\nth evaluate.lua -model demo-model_final.t7 -src_file data/src-val.txt -output_file pred.txt\n-src_dict data/demo.src.dict -targ_dict data/demo.targ.dict\n\n\n\n\nThis will output predictions into \npred.txt\n. The predictions are going to be quite terrible,\nas the demo dataset is small. Try running on some larger datasets! For example you can download\nmillions of parallel sentences for \ntranslation\n\nor \nsummarization\n.", 
            "title": "Quickstart"
        }, 
        {
            "location": "/Quickstart/#quickstart", 
            "text": "We are going to be working with some example data in  data/  folder.\nFirst run the data-processing code  th preprocess.lua -train_src_file data/src-train.txt -train_targ_file data/targ-train.txt\n    -valid_src_file data/src-val.txt -valid_targ_file data/targ-val.txt -output_file data/demo  This will take the source/target train/valid files ( src-train.txt, targ-train.txt,\nsrc-val.txt, targ-val.txt ) and build the following files:   demo.src.dict : Dictionary of source vocab to index mappings.  demo.targ.dict : Dictionary of target vocab to index mappings.  demo-train.t7 : serialized torch file containing vocabulary, training and validation data   The  *.dict  files are needed to check vocabulary, or to preprocess data with fixed vocabularies  Now run the model  th train.lua -data_file data/demo-train.t7 -savefile demo-model  This will run the default model, which consists of a 2-layer LSTM with 500 hidden units\non both the encoder/decoder.\nYou can also add  -gpuid 1  to use (say) GPU 1 in the cluster.  Now you have a model which you can use to predict on new data. To do this we are\ngoing to be running beam search  th evaluate.lua -model demo-model_final.t7 -src_file data/src-val.txt -output_file pred.txt\n-src_dict data/demo.src.dict -targ_dict data/demo.targ.dict  This will output predictions into  pred.txt . The predictions are going to be quite terrible,\nas the demo dataset is small. Try running on some larger datasets! For example you can download\nmillions of parallel sentences for  translation \nor  summarization .", 
            "title": "Quickstart"
        }, 
        {
            "location": "/code/eval/", 
            "text": "", 
            "title": "Home"
        }, 
        {
            "location": "/code/eval/lib+eval+beam/", 
            "text": "onmt.Beam\n\n\nClass for managing the beam search process. \n\n\n[src]\n\n\n\n\nonmt.Beam(size)\n\n\nConstructor\n\n\nParameters:\n\n\n\n\nsize\n : The beam \nK\n.\n\n\n\n\n[src]\n\n\n\n\nonmt.Beam:get_current_state()\n\n\nGet the outputs for the current timestep.\n\n\n[src]\n\n\n\n\nonmt.Beam:get_current_origin()\n\n\nGet the backpointers for the current timestep.\n\n\n[src]\n\n\n\n\nonmt.Beam:advance(out, attn_out)\n\n\nGiven prob over words for every last beam \nout\n and attention\n \nattn_out\n. Compute and update the beam search.\n\n\nParameters:\n\n\n\n\nout\n- probs at the last step\n\n\nattn_out\n- attention at the last step\n\n\n\n\nReturns: true if beam search is complete.\n\n\n[src]\n\n\n\n\nonmt.Beam:get_hyp(k)\n\n\nWalk back to construct the full hypothesis \nk\n.\n\n\nParameters:\n\n\n\n\nk\n - the position in the beam to construct.\n\n\n\n\nReturns:\n\n\n\n\nThe hypothesis\n\n\nThe attention at each time step.\n\n\n\n\nUndocumented methods\n\n\n\n * \nonmt.Beam:sort_best()\n\n\n\n * \nonmt.Beam:get_best()", 
            "title": "Lib+eval+beam"
        }, 
        {
            "location": "/code/eval/lib+eval+beam/#onmtbeam", 
            "text": "Class for managing the beam search process.   [src]", 
            "title": "onmt.Beam"
        }, 
        {
            "location": "/code/eval/lib+eval+beam/#onmtbeamsize", 
            "text": "Constructor  Parameters:   size  : The beam  K .   [src]", 
            "title": "onmt.Beam(size)"
        }, 
        {
            "location": "/code/eval/lib+eval+beam/#onmtbeamget_current_state", 
            "text": "Get the outputs for the current timestep.  [src]", 
            "title": "onmt.Beam:get_current_state()"
        }, 
        {
            "location": "/code/eval/lib+eval+beam/#onmtbeamget_current_origin", 
            "text": "Get the backpointers for the current timestep.  [src]", 
            "title": "onmt.Beam:get_current_origin()"
        }, 
        {
            "location": "/code/eval/lib+eval+beam/#onmtbeamadvanceout-attn_out", 
            "text": "Given prob over words for every last beam  out  and attention\n  attn_out . Compute and update the beam search.  Parameters:   out - probs at the last step  attn_out - attention at the last step   Returns: true if beam search is complete.  [src]", 
            "title": "onmt.Beam:advance(out, attn_out)"
        }, 
        {
            "location": "/code/eval/lib+eval+beam/#onmtbeamget_hypk", 
            "text": "Walk back to construct the full hypothesis  k .  Parameters:   k  - the position in the beam to construct.   Returns:   The hypothesis  The attention at each time step.", 
            "title": "onmt.Beam:get_hyp(k)"
        }, 
        {
            "location": "/code/eval/lib+eval+beam/#undocumented-methods", 
            "text": "*  onmt.Beam:sort_best()  \n *  onmt.Beam:get_best()", 
            "title": "Undocumented methods"
        }, 
        {
            "location": "/code/onmt/", 
            "text": "Package", 
            "title": "Home"
        }, 
        {
            "location": "/code/onmt/lib+onmt+BiEncoder/", 
            "text": "onmt.BiEncoder\n\n\nBiEncoder is a bidirectional Sequencer used for the source language.\n\n\nnet_fwd\n\n\nh_1 =\n h_2 =\n h_3 =\n ... =\n h_n\n |      |      |             |\n .      .      .             .\n |      |      |             |\nh_1 =\n h_2 =\n h_3 =\n ... =\n h_n\n |      |      |             |\n |      |      |             |\nx_1    x_2    x_3           x_n\n\n\n\nnet_bwd\n\n\nh_1 \n= h_2 \n= h_3 \n= ... \n= h_n\n |      |      |             |\n .      .      .             .\n |      |      |             |\nh_1 \n= h_2 \n= h_3 \n= ... \n= h_n\n |      |      |             |\n |      |      |             |\nx_1    x_2    x_3           x_n\n\n\n\nInherits from \nonmt.Sequencer\n.\n\n\n[src]\n\n\n\n\nonmt.BiEncoder(args, merge, net_fwd, net_bwd)\n\n\nCreates two Encoder's (encoder.lua) \nnet_fwd\n and \nnet_bwd\n.\n  The two are combined use \nmerge\n operation (concat/sum).\n\n\nUndocumented methods\n\n\n\n * \nonmt.BiEncoder:forward(batch)\n\n\n\n * \nonmt.BiEncoder:backward(batch, grad_states_output, grad_context_output)", 
            "title": "lib+onmt+BiEncoder"
        }, 
        {
            "location": "/code/onmt/lib+onmt+BiEncoder/#onmtbiencoder", 
            "text": "BiEncoder is a bidirectional Sequencer used for the source language.  net_fwd  h_1 =  h_2 =  h_3 =  ... =  h_n\n |      |      |             |\n .      .      .             .\n |      |      |             |\nh_1 =  h_2 =  h_3 =  ... =  h_n\n |      |      |             |\n |      |      |             |\nx_1    x_2    x_3           x_n  net_bwd  h_1  = h_2  = h_3  = ...  = h_n\n |      |      |             |\n .      .      .             .\n |      |      |             |\nh_1  = h_2  = h_3  = ...  = h_n\n |      |      |             |\n |      |      |             |\nx_1    x_2    x_3           x_n  Inherits from  onmt.Sequencer .  [src]", 
            "title": "onmt.BiEncoder"
        }, 
        {
            "location": "/code/onmt/lib+onmt+BiEncoder/#onmtbiencoderargs-merge-net_fwd-net_bwd", 
            "text": "Creates two Encoder's (encoder.lua)  net_fwd  and  net_bwd .\n  The two are combined use  merge  operation (concat/sum).", 
            "title": "onmt.BiEncoder(args, merge, net_fwd, net_bwd)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+BiEncoder/#undocumented-methods", 
            "text": "*  onmt.BiEncoder:forward(batch)  \n *  onmt.BiEncoder:backward(batch, grad_states_output, grad_context_output)", 
            "title": "Undocumented methods"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Decoder/", 
            "text": "onmt.Decoder\n\n\nDecoder is the sequencer for the target words.\n\n\n .      .      .             .\n |      |      |             |\nh_1 =\n h_2 =\n h_3 =\n ... =\n h_n\n |      |      |             |\n .      .      .             .\n |      |      |             |\nh_1 =\n h_2 =\n h_3 =\n ... =\n h_n\n |      |      |             |\n |      |      |             |\nx_1    x_2    x_3           x_n\n\n\n\nInherits from \nonmt.Sequencer\n.\n\n\n[src]\n\n\n\n\nonmt.Decoder(args, network, generator)\n\n\nConstruct an encoder layer. \n\n\nParameters:\n\n\n\n\nargs\n - global options.\n\n\nnetwork\n - optional, recurrent step template.\n\n\ngenerator\n - optional, a output \nonmt.Generator\n.\n\n\n\n\n[src]\n\n\n\n\nonmt.Decoder:reset(source_sizes, source_length, beam_size)\n\n\nUpdate internals of model to prepare for new batch.\n\n\nParameters:\n\n\n\n\nSee  \nonmt.MaskedSoftmax\n.\n\n\n\n\n[src]\n\n\n\n\nonmt.Decoder:forward_one(input, prev_states, context, prev_out, t)\n\n\nRun one step of the decoder.\n\n\nParameters:\n\n\n\n\ninput\n - sparse input (1)\n\n\nprev_states\n - stack of hidden states (batch x layers*model x rnn_size)\n\n\ncontext\n - encoder output (batch x n x rnn_size)\n\n\nprev_out\n - previous distribution (batch x #words)\n\n\nt\n - current timestep\n\n\n\n\nReturns:\n\n\n\n\nout\n - Top-layer Hidden state\n\n\nstates\n - All states\n\n\n\n\n[src]\n\n\n\n\nonmt.Decoder:forward(batch, encoder_states, context)\n\n\nCompute all forward steps.\n\n\nParameters:\n\n\n\n\nbatch\n - based on data.lua\n\n\nencoder_states\n - the final encoder states\n\n\ncontext\n - the context to apply attention to.\n\n\n\n\nReturns: Tables of top hidden layer at each timestep.\n\n\n[src]\n\n\n\n\nonmt.Decoder:compute_score(batch, encoder_states, context)\n\n\nCompute the cumulative score of a target sequence.\n  Used in decoding when gold data are provided.\n\n\n[src]\n\n\n\n\nonmt.Decoder:compute_loss(batch, encoder_states, context, criterion)\n\n\nCompute the loss on a batch based on final layer \ngenerator\n.\n\n\n[src]\n\n\n\n\nonmt.Decoder:backward(batch, outputs, criterion)\n\n\nCompute the standard backward update.\n\n\nParameters:\n\n\n\n\nbatch\n\n\noutputs\n\n\ncriterion\n\n\n\n\nNote: This code is both the standard backward and criterion forward/backward.\n  It returns both the gradInputs (ret 1 and 2) and the loss.\n\n\nUndocumented methods\n\n\n\n * \nonmt.Decoder:forward_and_apply(batch, encoder_states, context, func)", 
            "title": "lib+onmt+Decoder"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Decoder/#onmtdecoder", 
            "text": "Decoder is the sequencer for the target words.   .      .      .             .\n |      |      |             |\nh_1 =  h_2 =  h_3 =  ... =  h_n\n |      |      |             |\n .      .      .             .\n |      |      |             |\nh_1 =  h_2 =  h_3 =  ... =  h_n\n |      |      |             |\n |      |      |             |\nx_1    x_2    x_3           x_n  Inherits from  onmt.Sequencer .  [src]", 
            "title": "onmt.Decoder"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Decoder/#onmtdecoderargs-network-generator", 
            "text": "Construct an encoder layer.   Parameters:   args  - global options.  network  - optional, recurrent step template.  generator  - optional, a output  onmt.Generator .   [src]", 
            "title": "onmt.Decoder(args, network, generator)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Decoder/#onmtdecoderresetsource_sizes-source_length-beam_size", 
            "text": "Update internals of model to prepare for new batch.  Parameters:   See   onmt.MaskedSoftmax .   [src]", 
            "title": "onmt.Decoder:reset(source_sizes, source_length, beam_size)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Decoder/#onmtdecoderforward_oneinput-prev_states-context-prev_out-t", 
            "text": "Run one step of the decoder.  Parameters:   input  - sparse input (1)  prev_states  - stack of hidden states (batch x layers*model x rnn_size)  context  - encoder output (batch x n x rnn_size)  prev_out  - previous distribution (batch x #words)  t  - current timestep   Returns:   out  - Top-layer Hidden state  states  - All states   [src]", 
            "title": "onmt.Decoder:forward_one(input, prev_states, context, prev_out, t)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Decoder/#onmtdecoderforwardbatch-encoder_states-context", 
            "text": "Compute all forward steps.  Parameters:   batch  - based on data.lua  encoder_states  - the final encoder states  context  - the context to apply attention to.   Returns: Tables of top hidden layer at each timestep.  [src]", 
            "title": "onmt.Decoder:forward(batch, encoder_states, context)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Decoder/#onmtdecodercompute_scorebatch-encoder_states-context", 
            "text": "Compute the cumulative score of a target sequence.\n  Used in decoding when gold data are provided.  [src]", 
            "title": "onmt.Decoder:compute_score(batch, encoder_states, context)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Decoder/#onmtdecodercompute_lossbatch-encoder_states-context-criterion", 
            "text": "Compute the loss on a batch based on final layer  generator .  [src]", 
            "title": "onmt.Decoder:compute_loss(batch, encoder_states, context, criterion)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Decoder/#onmtdecoderbackwardbatch-outputs-criterion", 
            "text": "Compute the standard backward update.  Parameters:   batch  outputs  criterion   Note: This code is both the standard backward and criterion forward/backward.\n  It returns both the gradInputs (ret 1 and 2) and the loss.", 
            "title": "onmt.Decoder:backward(batch, outputs, criterion)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Decoder/#undocumented-methods", 
            "text": "*  onmt.Decoder:forward_and_apply(batch, encoder_states, context, func)", 
            "title": "Undocumented methods"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Encoder/", 
            "text": "onmt.Encoder\n\n\nEncoder is a unidirectional Sequencer used for the source language.\n\n\nh_1 =\n h_2 =\n h_3 =\n ... =\n h_n\n |      |      |             |\n .      .      .             .\n |      |      |             |\nh_1 =\n h_2 =\n h_3 =\n ... =\n h_n\n |      |      |             |\n |      |      |             |\nx_1    x_2    x_3           x_n\n\n\n\nInherits from \nonmt.Sequencer\n.\n\n\n[src]\n\n\n\n\nonmt.Encoder(args, network)\n\n\nConstruct an encoder layer. \n\n\nParameters:\n\n\n\n\nargs\n - global options.\n\n\nnetwork\n - optional recurrent step template.\n\n\n\n\n[src]\n\n\n\n\nonmt.Encoder:forward(batch)\n\n\nCompute the context representation of an input.\n\n\nParameters:\n\n\n\n\nbatch\n - a \nbatch struct\n as defined data.lua.\n\n\n\n\nReturns:\n\n\n\n\n\n\n\n\nfinal hidden states\n\n\n\n\n\n\n\n\n\n\ncontext matrix H\n\n\n\n\n\n\n\n\n[src]\n\n\n\n\nonmt.Encoder:backward(batch, grad_states_output, grad_context_output)\n\n\nBackward pass (only called during training)\n\n\nParameters:\n\n\n\n\nbatch\n - must be same as for forward\n\n\ngrad_states_output\n gradient of loss wrt last state\n\n\ngrad_context_output\n - gradient of loss wrt full context.\n\n\n\n\nReturns: nil", 
            "title": "lib+onmt+Encoder"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Encoder/#onmtencoder", 
            "text": "Encoder is a unidirectional Sequencer used for the source language.  h_1 =  h_2 =  h_3 =  ... =  h_n\n |      |      |             |\n .      .      .             .\n |      |      |             |\nh_1 =  h_2 =  h_3 =  ... =  h_n\n |      |      |             |\n |      |      |             |\nx_1    x_2    x_3           x_n  Inherits from  onmt.Sequencer .  [src]", 
            "title": "onmt.Encoder"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Encoder/#onmtencoderargs-network", 
            "text": "Construct an encoder layer.   Parameters:   args  - global options.  network  - optional recurrent step template.   [src]", 
            "title": "onmt.Encoder(args, network)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Encoder/#onmtencoderforwardbatch", 
            "text": "Compute the context representation of an input.  Parameters:   batch  - a  batch struct  as defined data.lua.   Returns:     final hidden states      context matrix H     [src]", 
            "title": "onmt.Encoder:forward(batch)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Encoder/#onmtencoderbackwardbatch-grad_states_output-grad_context_output", 
            "text": "Backward pass (only called during training)  Parameters:   batch  - must be same as for forward  grad_states_output  gradient of loss wrt last state  grad_context_output  - gradient of loss wrt full context.   Returns: nil", 
            "title": "onmt.Encoder:backward(batch, grad_states_output, grad_context_output)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+GlobalAttention/", 
            "text": "onmt.GlobalAttention\n\n\nGlobal attention takes a matrix and a query vector. It \nthen computes a parameterized convex combination of the matrix \nbased on the input query. \n\n\nH_1 H_2 H_3 ... H_n\n q   q   q       q\n  |  |   |       |   \n   \\ |   |      /\n       .....\n     \\   |  /\n         a\n\n\n\n[src]\n\n\n\n\nonmt.GlobalAttention(dim)\n\n\nA nn-style module computing attention.\n\n\nConstructs a unit mapping:\n  \n(H_1 .. H_n, q) => (a)\n\n  Where H is of \nbatch x n x dim\n and q is of \nbatch x dim\n.\n\n\nThe full function is  \n\\tanh(W_2 [(softmax((W_1 q + b_1) H) H), q] + b_2)\n.\n\n\nParameters:\n\n\n\n\ndim\n - dimension of the context vectors.\n\n\n\n\nUndocumented methods\n\n\n\n * \nonmt.GlobalAttention:updateOutput(input)\n\n\n\n * \nonmt.GlobalAttention:updateGradInput(input, gradOutput)\n\n\n\n * \nonmt.GlobalAttention:accGradParameters(input, gradOutput, scale)", 
            "title": "lib+onmt+GlobalAttention"
        }, 
        {
            "location": "/code/onmt/lib+onmt+GlobalAttention/#onmtglobalattention", 
            "text": "Global attention takes a matrix and a query vector. It \nthen computes a parameterized convex combination of the matrix \nbased on the input query.   H_1 H_2 H_3 ... H_n\n q   q   q       q\n  |  |   |       |   \n   \\ |   |      /\n       .....\n     \\   |  /\n         a  [src]", 
            "title": "onmt.GlobalAttention"
        }, 
        {
            "location": "/code/onmt/lib+onmt+GlobalAttention/#onmtglobalattentiondim", 
            "text": "A nn-style module computing attention.  Constructs a unit mapping:\n   (H_1 .. H_n, q) => (a) \n  Where H is of  batch x n x dim  and q is of  batch x dim .  The full function is   \\tanh(W_2 [(softmax((W_1 q + b_1) H) H), q] + b_2) .  Parameters:   dim  - dimension of the context vectors.", 
            "title": "onmt.GlobalAttention(dim)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+GlobalAttention/#undocumented-methods", 
            "text": "*  onmt.GlobalAttention:updateOutput(input)  \n *  onmt.GlobalAttention:updateGradInput(input, gradOutput)  \n *  onmt.GlobalAttention:accGradParameters(input, gradOutput, scale)", 
            "title": "Undocumented methods"
        }, 
        {
            "location": "/code/onmt/lib+onmt+LSTM/", 
            "text": "onmt.LSTM\n\n\n[src]\n\n\n\n\nonmt.LSTM(num_layers, input_size, hidden_size, dropout)\n\n\nA nn-style module computing one LSTM step.\n\n\nComputes \n(c_{t-1}, h_{t-1}, x_t) => (c_{t}, h_{t})\n.\n\n\nParameters:\n\n\n\n\nnum_layers\n - Number of LSTM layers.  \n\n\ninput_size\n -  Size of input layer x.\n\n\nhidden_size\n -  Size of the hidden layers (cell and hidden).\n\n\ndropout\n - Dropout rate to use.\n\n\n\n\nUndocumented methods\n\n\n\n * \nonmt.LSTM:updateOutput(input)\n\n\n\n * \nonmt.LSTM:updateGradInput(input, gradOutput)\n\n\n\n * \nonmt.LSTM:accGradParameters(input, gradOutput, scale)", 
            "title": "lib+onmt+LSTM"
        }, 
        {
            "location": "/code/onmt/lib+onmt+LSTM/#onmtlstm", 
            "text": "[src]", 
            "title": "onmt.LSTM"
        }, 
        {
            "location": "/code/onmt/lib+onmt+LSTM/#onmtlstmnum_layers-input_size-hidden_size-dropout", 
            "text": "A nn-style module computing one LSTM step.  Computes  (c_{t-1}, h_{t-1}, x_t) => (c_{t}, h_{t}) .  Parameters:   num_layers  - Number of LSTM layers.    input_size  -  Size of input layer x.  hidden_size  -  Size of the hidden layers (cell and hidden).  dropout  - Dropout rate to use.", 
            "title": "onmt.LSTM(num_layers, input_size, hidden_size, dropout)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+LSTM/#undocumented-methods", 
            "text": "*  onmt.LSTM:updateOutput(input)  \n *  onmt.LSTM:updateGradInput(input, gradOutput)  \n *  onmt.LSTM:accGradParameters(input, gradOutput, scale)", 
            "title": "Undocumented methods"
        }, 
        {
            "location": "/code/onmt/lib+onmt+MaskedSoftmax/", 
            "text": "onmt.MaskedSoftmax\n\n\n[src]\n\n\n\n\nonmt.MaskedSoftmax(source_sizes, source_length, beam_size)\n\n\nA nn-style module that applies a softmax on input that gives no weight to the left padding.\n\n\nParameters:\n\n\n\n\nsource_sizes\n -  the true lengths (with left padding).\n\n\nsource_length\n - the max length in the batch \nbeam_size\n.\n\n\nbeam_size\n - beam size ${K}\n\n\n\n\nUndocumented methods\n\n\n\n * \nonmt.MaskedSoftmax:updateOutput(input)\n\n\n\n * \nonmt.MaskedSoftmax:updateGradInput(input, gradOutput)\n\n\n\n * \nonmt.MaskedSoftmax:accGradParameters(input, gradOutput, scale)", 
            "title": "lib+onmt+MaskedSoftmax"
        }, 
        {
            "location": "/code/onmt/lib+onmt+MaskedSoftmax/#onmtmaskedsoftmax", 
            "text": "[src]", 
            "title": "onmt.MaskedSoftmax"
        }, 
        {
            "location": "/code/onmt/lib+onmt+MaskedSoftmax/#onmtmaskedsoftmaxsource_sizes-source_length-beam_size", 
            "text": "A nn-style module that applies a softmax on input that gives no weight to the left padding.  Parameters:   source_sizes  -  the true lengths (with left padding).  source_length  - the max length in the batch  beam_size .  beam_size  - beam size ${K}", 
            "title": "onmt.MaskedSoftmax(source_sizes, source_length, beam_size)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+MaskedSoftmax/#undocumented-methods", 
            "text": "*  onmt.MaskedSoftmax:updateOutput(input)  \n *  onmt.MaskedSoftmax:updateGradInput(input, gradOutput)  \n *  onmt.MaskedSoftmax:accGradParameters(input, gradOutput, scale)", 
            "title": "Undocumented methods"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Sequencer/", 
            "text": "onmt.Sequencer\n\n\nSequencer is the base class for encoder and decoder models.\n  Main task is to manage \nself.net(t)\n, the unrolled network\n  used during training.\n\n\n[src]\n\n\n\n\nonmt.Sequencer(args, network)\n\n\nConstructor\n\n\nParameters:\n\n\n\n\nargs\n - global options.\n\n\nnetwork\n - optional recurrent step template.\n\n\n\n\n[src]\n\n\n\n\nonmt.Sequencer:net(t)\n\n\nGet access to the recurrent unit at a timestep.\n\n\nParameters:\n  * \nt\n - timestep.\n\n\nReturns: The raw network clone at timestep t.\n  When \nevaluate()\n has been called, cheat and return t=1.\n\n\n[src]\n\n\n\n\nonmt.Sequencer:training()\n\n\nMove the network to train mode. \n\n\n[src]\n\n\n\n\nonmt.Sequencer:evaluate()\n\n\nMove the network to evaluation mode.", 
            "title": "lib+onmt+Sequencer"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Sequencer/#onmtsequencer", 
            "text": "Sequencer is the base class for encoder and decoder models.\n  Main task is to manage  self.net(t) , the unrolled network\n  used during training.  [src]", 
            "title": "onmt.Sequencer"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Sequencer/#onmtsequencerargs-network", 
            "text": "Constructor  Parameters:   args  - global options.  network  - optional recurrent step template.   [src]", 
            "title": "onmt.Sequencer(args, network)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Sequencer/#onmtsequencernett", 
            "text": "Get access to the recurrent unit at a timestep.  Parameters:\n  *  t  - timestep.  Returns: The raw network clone at timestep t.\n  When  evaluate()  has been called, cheat and return t=1.  [src]", 
            "title": "onmt.Sequencer:net(t)"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Sequencer/#onmtsequencertraining", 
            "text": "Move the network to train mode.   [src]", 
            "title": "onmt.Sequencer:training()"
        }, 
        {
            "location": "/code/onmt/lib+onmt+Sequencer/#onmtsequencerevaluate", 
            "text": "Move the network to evaluation mode.", 
            "title": "onmt.Sequencer:evaluate()"
        }, 
        {
            "location": "/code/onmt/lib+onmt+WordEmbedding/", 
            "text": "onmt.WordEmbedding\n\n\nUndocumented methods\n\n\n\n * \nonmt.WordEmbedding(vocab_size, vec_size, pre_trained, fix)\n\n\n\n * \nonmt.WordEmbedding:updateOutput(input)\n\n\n\n * \nonmt.WordEmbedding:updateGradInput(input, gradOutput)\n\n\n\n * \nonmt.WordEmbedding:accGradParameters(input, gradOutput, scale)", 
            "title": "lib+onmt+WordEmbedding"
        }, 
        {
            "location": "/code/onmt/lib+onmt+WordEmbedding/#onmtwordembedding", 
            "text": "", 
            "title": "onmt.WordEmbedding"
        }, 
        {
            "location": "/code/onmt/lib+onmt+WordEmbedding/#undocumented-methods", 
            "text": "*  onmt.WordEmbedding(vocab_size, vec_size, pre_trained, fix)  \n *  onmt.WordEmbedding:updateOutput(input)  \n *  onmt.WordEmbedding:updateGradInput(input, gradOutput)  \n *  onmt.WordEmbedding:accGradParameters(input, gradOutput, scale)", 
            "title": "Undocumented methods"
        }, 
        {
            "location": "/code/train/", 
            "text": "", 
            "title": "Home"
        }, 
        {
            "location": "/code/train/lib+data/", 
            "text": "onmt.Data\n\n\nData management and batch creation.\n\n\nBatch interface [size]:\n\n\n\n\nsize: number of sentences in the batch [1]\n\n\nsource_length: max length in source batch [1]\n\n\nsource_size:  lengths of each source [batch x 1]\n\n\nsource_input:  left-padded idx's of source (PPPPPPABCDE) [batch x max]\n\n\nsource_input_rev: right-padded  idx's of source rev (EDCBAPPPPPP) [batch x max]\n\n\ntarget_length: max length in source batch [1]\n\n\ntarget_size: lengths of each source [batch x 1]\n\n\ntarget_non_zeros: number of non-ignored words in batch [1]\n\n\ntarget_input: input idx's of target (SABCDEPPPPPP) [batch x max]\n\n\ntarget_output: expected output idx's of target (ABCDESPPPPPP) [batch x max]\n\n\n\n\nTODO: change name of size =\n maxlen\n\n\n[src]\n\n\n\n\nonmt.Data(src, targ)\n\n\nInitialize a data object given aligned tables of IntTensors \nsrc\n\n  and \ntarg\n.\n\n\n[src]\n\n\n\n\nonmt.Data:set_batch_size(max_batch_size)\n\n\nSetup up the training data to respect \nmax_batch_size\n. \n\n\n[src]\n\n\n\n\nonmt.Data:get_data(src, targ, nocuda)\n\n\nCreate a batch object given aligned sent tables \nsrc\n and \ntarg\n\n  (optional). Data format is shown at the top of the file.\n\n\n[src]\n\n\n\n\nonmt.Data:get_batch(idx, nocuda)\n\n\nGet batch \nidx\n. If nil make a batch of all the data. \n\n\n[src]\n\n\n\n\nonmt.Data:distribute(batch, count)\n\n\nSlice batch into several smaller batcher for data parallelism.", 
            "title": "Lib+data"
        }, 
        {
            "location": "/code/train/lib+data/#onmtdata", 
            "text": "Data management and batch creation.  Batch interface [size]:   size: number of sentences in the batch [1]  source_length: max length in source batch [1]  source_size:  lengths of each source [batch x 1]  source_input:  left-padded idx's of source (PPPPPPABCDE) [batch x max]  source_input_rev: right-padded  idx's of source rev (EDCBAPPPPPP) [batch x max]  target_length: max length in source batch [1]  target_size: lengths of each source [batch x 1]  target_non_zeros: number of non-ignored words in batch [1]  target_input: input idx's of target (SABCDEPPPPPP) [batch x max]  target_output: expected output idx's of target (ABCDESPPPPPP) [batch x max]   TODO: change name of size =  maxlen  [src]", 
            "title": "onmt.Data"
        }, 
        {
            "location": "/code/train/lib+data/#onmtdatasrc-targ", 
            "text": "Initialize a data object given aligned tables of IntTensors  src \n  and  targ .  [src]", 
            "title": "onmt.Data(src, targ)"
        }, 
        {
            "location": "/code/train/lib+data/#onmtdataset_batch_sizemax_batch_size", 
            "text": "Setup up the training data to respect  max_batch_size .   [src]", 
            "title": "onmt.Data:set_batch_size(max_batch_size)"
        }, 
        {
            "location": "/code/train/lib+data/#onmtdataget_datasrc-targ-nocuda", 
            "text": "Create a batch object given aligned sent tables  src  and  targ \n  (optional). Data format is shown at the top of the file.  [src]", 
            "title": "onmt.Data:get_data(src, targ, nocuda)"
        }, 
        {
            "location": "/code/train/lib+data/#onmtdataget_batchidx-nocuda", 
            "text": "Get batch  idx . If nil make a batch of all the data.   [src]", 
            "title": "onmt.Data:get_batch(idx, nocuda)"
        }, 
        {
            "location": "/code/train/lib+data/#onmtdatadistributebatch-count", 
            "text": "Slice batch into several smaller batcher for data parallelism.", 
            "title": "onmt.Data:distribute(batch, count)"
        }, 
        {
            "location": "/code/train/lib+train+checkpoint/", 
            "text": "onmt.Checkpoint\n\n\nClass for saving and loading models during training.\n\n\n[src]\n\n\n\n\nonmt.Checkpoint:save_iteration(iteration, epoch_state, batch_order)\n\n\nSave the model and data in the middle of an epoch sorting the iteration. \n\n\nUndocumented methods\n\n\n\n * \nonmt.Checkpoint(options, nets, optim, dataset)\n\n\n\n * \nonmt.Checkpoint:save(file_path, info)\n\n\n\n * \nonmt.Checkpoint:save_epoch(valid_ppl, epoch_state)", 
            "title": "Lib+train+checkpoint"
        }, 
        {
            "location": "/code/train/lib+train+checkpoint/#onmtcheckpoint", 
            "text": "Class for saving and loading models during training.  [src]", 
            "title": "onmt.Checkpoint"
        }, 
        {
            "location": "/code/train/lib+train+checkpoint/#onmtcheckpointsave_iterationiteration-epoch_state-batch_order", 
            "text": "Save the model and data in the middle of an epoch sorting the iteration.", 
            "title": "onmt.Checkpoint:save_iteration(iteration, epoch_state, batch_order)"
        }, 
        {
            "location": "/code/train/lib+train+checkpoint/#undocumented-methods", 
            "text": "*  onmt.Checkpoint(options, nets, optim, dataset)  \n *  onmt.Checkpoint:save(file_path, info)  \n *  onmt.Checkpoint:save_epoch(valid_ppl, epoch_state)", 
            "title": "Undocumented methods"
        }, 
        {
            "location": "/code/train/lib+train+epoch_state/", 
            "text": "onmt.EpochState\n\n\nClass for managing the training process by logging and storing\n  the state of the current epoch.\n\n\n[src]\n\n\n\n\nonmt.EpochState(epoch, status)\n\n\nInitialize for epoch \nepoch\n and training \nstatus\n (current loss)\n\n\n[src]\n\n\n\n\nonmt.EpochState:update(batches, losses)\n\n\nUpdate training status. Takes \nbatch\n (described in data.lua) and last losses.\n\n\n[src]\n\n\n\n\nonmt.EpochState:log(batch_index, data_size, learning_rate)\n\n\nLog to status stdout.\n  TODO: these args shouldn't need to be passed in each time. \n\n\nUndocumented methods\n\n\n\n * \nonmt.EpochState:get_train_ppl()\n\n\n\n * \nonmt.EpochState:get_time()\n\n\n\n * \nonmt.EpochState:get_status()", 
            "title": "Lib+train+epoch state"
        }, 
        {
            "location": "/code/train/lib+train+epoch_state/#onmtepochstate", 
            "text": "Class for managing the training process by logging and storing\n  the state of the current epoch.  [src]", 
            "title": "onmt.EpochState"
        }, 
        {
            "location": "/code/train/lib+train+epoch_state/#onmtepochstateepoch-status", 
            "text": "Initialize for epoch  epoch  and training  status  (current loss)  [src]", 
            "title": "onmt.EpochState(epoch, status)"
        }, 
        {
            "location": "/code/train/lib+train+epoch_state/#onmtepochstateupdatebatches-losses", 
            "text": "Update training status. Takes  batch  (described in data.lua) and last losses.  [src]", 
            "title": "onmt.EpochState:update(batches, losses)"
        }, 
        {
            "location": "/code/train/lib+train+epoch_state/#onmtepochstatelogbatch_index-data_size-learning_rate", 
            "text": "Log to status stdout.\n  TODO: these args shouldn't need to be passed in each time.", 
            "title": "onmt.EpochState:log(batch_index, data_size, learning_rate)"
        }, 
        {
            "location": "/code/train/lib+train+epoch_state/#undocumented-methods", 
            "text": "*  onmt.EpochState:get_train_ppl()  \n *  onmt.EpochState:get_time()  \n *  onmt.EpochState:get_status()", 
            "title": "Undocumented methods"
        }, 
        {
            "location": "/code/train/lib+train+optim/", 
            "text": "onmt.Optim\n\n\n[src]\n\n\n\n\nonmt.Optim:update_learning_rate(score, epoch)\n\n\ndecay learning rate if val perf does not improve or we hit the start_decay_at limit\n\n\nUndocumented methods\n\n\n\n * \nonmt.Optim(args)\n\n\n\n * \nonmt.Optim:zero_grad(grad_params)\n\n\n\n * \nonmt.Optim:update_params(params, grad_params, max_grad_norm)\n\n\n\n * \nonmt.Optim:get_learning_rate()\n\n\n\n * \nonmt.Optim:get_states()", 
            "title": "Lib+train+optim"
        }, 
        {
            "location": "/code/train/lib+train+optim/#onmtoptim", 
            "text": "[src]", 
            "title": "onmt.Optim"
        }, 
        {
            "location": "/code/train/lib+train+optim/#onmtoptimupdate_learning_ratescore-epoch", 
            "text": "decay learning rate if val perf does not improve or we hit the start_decay_at limit", 
            "title": "onmt.Optim:update_learning_rate(score, epoch)"
        }, 
        {
            "location": "/code/train/lib+train+optim/#undocumented-methods", 
            "text": "*  onmt.Optim(args)  \n *  onmt.Optim:zero_grad(grad_params)  \n *  onmt.Optim:update_params(params, grad_params, max_grad_norm)  \n *  onmt.Optim:get_learning_rate()  \n *  onmt.Optim:get_states()", 
            "title": "Undocumented methods"
        }, 
        {
            "location": "/details/evaluate/", 
            "text": "evaluate.lua\n\n\n\n\nconfig\n\n\nRead options from this file []\n\n\n\n\nData options\n\n\n\n\nmodel\n\n\nPath to model .t7 file []\n\n\nsrc_file\n\n\nSource sequence to decode (one line per sequence) []\n\n\ntarg_file\n\n\nTrue target sequence (optional) []\n\n\noutput_file\n\n\nPath to output the predictions (each line will be the decoded sequence [pred.txt]\n\n\n\n\nBeam Search options\n\n\n\n\nbeam\n\n\nBeam size [5]\n\n\nbatch\n\n\nBatch size [30]\n\n\nmax_sent_l\n\n\nMaximum sentence length. If any sequences in srcfile are longer than this then it will error out [250]\n\n\nreplace_unk\n\n\nReplace the generated UNK tokens with the source token thathad the highest attention weight. If srctarg_dict is provided,it will lookup the identified source token and give the correspondingtarget token. If it is not provided (or the identified source tokendoes not exist in the table) then it will copy the source token [false]\n\n\nsrctarg_dict\n\n\nPath to source-target dictionary to replace UNKtokens. See README.md for the format this file should be in []\n\n\nn_best\n\n\nIf \n 1, it will also output an n_best list of decoded sentences [1]\n\n\n\n\nOther options\n\n\n\n\ngpuid\n\n\nID of the GPU to use (-1 = use CPU, 0 = let cuda choose between available GPUs) [-1]\n\n\nfallback_to_cpu\n\n\nIf = true, fallback to CPU if no GPU available [false]\n\n\ncudnn\n\n\nIf using character model, this should be true if the character model was trained using cudnn [false]", 
            "title": "Evaluate"
        }, 
        {
            "location": "/details/preprocess/", 
            "text": "Preprocess Options\n\n\n\n\nconfig\n\n\nRead options from this file []\n\n\ntrain_src_file\n\n\nPath to the training source data []\n\n\ntrain_targ_file\n\n\nPath to the training target data []\n\n\nvalid_src_file\n\n\nPath to the validation source data []\n\n\nvalid_targ_file\n\n\nPath to the validation target data []\n\n\noutput_file\n\n\nOutput file for the prepared data []\n\n\nsrc_vocab_size\n\n\nSize of the source vocabulary [50000]\n\n\ntarg_vocab_size\n\n\nSize of the target vocabulary [50000]\n\n\nsrc_vocab_file\n\n\nPre-calculated source vocabulary []\n\n\ntarg_vocab_file\n\n\nPre-calculated target vocabulary []\n\n\nfeatures_vocabs_prefix\n\n\nPath prefix to existing features vocabularies []\n\n\nseq_length\n\n\nMaximum sequence length [50]\n\n\nshuffle\n\n\nSuffle data [true]\n\n\nseed\n\n\nRandom seed [3435]", 
            "title": "Preprocess"
        }, 
        {
            "location": "/details/train/", 
            "text": "train.lua\n\n\n\n\nconfig\n\n\nRead options from this file []\n\n\n\n\nData options\n\n\n\n\ndata\n\n\nPath to the training *-train.t7 file from preprocess.lua []\n\n\nsave_file\n\n\nSavefile name (model will be saved assavefile_epochX_PPL.t7 where X is the X-th epoch and PPL isthe validation perplexity []\n\n\ntrain_from\n\n\nIf training from a checkpoint then this is the path to the pretrained model. []\n\n\ncontinue\n\n\nIf training from a checkpoint, whether to continue the training in the same configuration or not. [false]\n\n\n\n\nModel options\n\n\n\n\nnum_layers\n\n\nNumber of layers in the LSTM encoder/decoder [2]\n\n\nrnn_size\n\n\nSize of LSTM hidden states [500]\n\n\nword_vec_size\n\n\nWord embedding sizes [500]\n\n\nfeat_vec_exponent\n\n\nIf the feature takes N values, then theembedding dimension will be set to N^exponent [0.7]\n\n\ninput_feed\n\n\nFeed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder. [true]\n\n\nbrnn\n\n\nUse a bidirectional encoder [false]\n\n\nbrnn_merge\n\n\nMerge action for the bidirectional hidden states: concat or sum [sum]\n\n\n\n\nOptimization options\n\n\n\n\nmax_batch_size\n\n\nMaximum batch size [64]\n\n\nepochs\n\n\nNumber of training epochs [13]\n\n\nstart_epoch\n\n\nIf loading from a checkpoint, the epoch from which to start [1]\n\n\nstart_iteration\n\n\nIf loading from a checkpoint, the iteration from which to start [1]\n\n\nparam_init\n\n\nParameters are initialized over uniform distribution with support (-param_init, param_init) [0.1]\n\n\noptim\n\n\nOptimization method. Possible options are: sgd, adagrad, adadelta, adam [sgd]\n\n\nlearning_rate\n\n\nStarting learning rate. If adagrad/adadelta/adam is used,then this is the global learning rate. Recommended settings: sgd =1,adagrad = 0.1, adadelta = 1, adam = 0.1 [1]\n\n\nmax_grad_norm\n\n\nIf the norm of the gradient vector exceeds this renormalize it to have the norm equal to max_grad_norm [5]\n\n\ndropout\n\n\nDropout probability. Dropout is applied between vertical LSTM stacks. [0.3]\n\n\nlr_decay\n\n\nDecay learning rate by this much if (i) perplexity does not decreaseon the validation set or (ii) epoch has gone past the start_decay_at_limit [0.5]\n\n\nstart_decay_at\n\n\nStart decay after this epoch [9]\n\n\ncurriculum\n\n\nFor this many epochs, order the minibatches based on sourcesequence length. Sometimes setting this to 1 will increase convergence speed. [0]\n\n\npre_word_vecs_enc\n\n\nIf a valid path is specified, then this will loadpretrained word embeddings on the encoder side.See README for specific formatting instructions. []\n\n\npre_word_vecs_dec\n\n\nIf a valid path is specified, then this will loadpretrained word embeddings on the decoder side.See README for specific formatting instructions. []\n\n\nfix_word_vecs_enc\n\n\nFix word embeddings on the encoder side [false]\n\n\nfix_word_vecs_dec\n\n\nFix word embeddings on the decoder side [false]\n\n\n\n\nOther options\n\n\n\n\ngpuid\n\n\nWhich gpu to use (1-indexed). \n 1 = use CPU [-1]\n\n\nnparallel\n\n\nHow many parallel process [1]\n\n\ndisable_mem_optimization\n\n\nDisable sharing internal of internal buffers between clones - which is in general safe,except if you want to look inside clones for visualization purpose for instance. [false]\n\n\ncudnn\n\n\nWhether to use cudnn or not [false]\n\n\nsave_every\n\n\nSave intermediate models every this many iterations within an epoch.If = 0, will not save models within an epoch.  [0]\n\n\nprint_every\n\n\nPrint stats every this many iterations within an epoch. [50]", 
            "title": "Train"
        }
    ]
}